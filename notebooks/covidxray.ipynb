{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"covidxray.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyO+EA/GiXE6EaGVrqFfxYDu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1e6d70bdb21c4e8c9e3d732f5fdb014b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_424e99b9dea84a89abf5619ff5a6e87c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f908f9a409e74a7abd83983d48c6c080","IPY_MODEL_f7a0b87b5de344aa90229373b554387f"]}},"424e99b9dea84a89abf5619ff5a6e87c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f908f9a409e74a7abd83983d48c6c080":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d8e1123cf3b6495c94b08d1abc831859","_dom_classes":[],"description":"100%","_model_name":"IntProgressModel","bar_style":"success","max":102502400,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102502400,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4e99966b35f74de48e90d0847cf1100d"}},"f7a0b87b5de344aa90229373b554387f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b8e457a2474c419091c1c517dd240bfd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [00:06&lt;00:00, 15.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dea8b4aa2d3342f486870b4003c2f859"}},"d8e1123cf3b6495c94b08d1abc831859":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4e99966b35f74de48e90d0847cf1100d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8e457a2474c419091c1c517dd240bfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dea8b4aa2d3342f486870b4003c2f859":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"LehwxUTqthqS","colab_type":"code","outputId":"5b7040f3-a88a-4a7b-fe8f-5adc8861b12e","executionInfo":{"status":"ok","timestamp":1586784643243,"user_tz":-330,"elapsed":58191,"user":{"displayName":"Ishan Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjC7HQ3Ze_4nwWEuNncSdbObEvv62uZyisKn7YN=s64","userId":"07222164059474910029"}},"colab":{"base_uri":"https://localhost:8080/","height":258}},"source":["!sudo -H pip install Augmentor\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/yo'  #change dir to your project folder"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting Augmentor\n","  Downloading https://files.pythonhosted.org/packages/cb/79/861f38d5830cff631e30e33b127076bfef8ac98171e51daa06df0118c75f/Augmentor-0.2.8-py2.py3-none-any.whl\n","Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (0.16.0)\n","Requirement already satisfied: tqdm>=4.9.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (4.38.0)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (1.18.2)\n","Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (7.0.0)\n","Installing collected packages: Augmentor\n","Successfully installed Augmentor-0.2.8\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nTmAUrvXuLZ7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"5c821cc4-eb01-481c-f8c0-69c21d50d7c8","executionInfo":{"status":"ok","timestamp":1586784672175,"user_tz":-330,"elapsed":4847,"user":{"displayName":"Ishan Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjC7HQ3Ze_4nwWEuNncSdbObEvv62uZyisKn7YN=s64","userId":"07222164059474910029"}}},"source":["import os\n","import torch\n","import random\n","import torchvision\n","import pandas as pd\n","import torch.nn as nn\n","import torchvision.models as m\n","import torch.nn.functional as F\n","\n","def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","            \n","def initialize_model(use_pretrained=False, num_classes=2):\n","    model = m.resnet50(pretrained=use_pretrained)\n","    num_ftrs = model.fc.in_features\n","    model.fc = nn.Linear(num_ftrs, num_classes)\n","    input_size = 224\n","    return model, input_size\n","\n","import os\n","DIR = os.getcwd()\n","model_dir = DIR + '/gdrive/My Drive/yo/datasets/covid19_vs_normal'\n","#PATH_MODEL = f'{model_dir}/checkpoints/chk_resnet_50_epoch_14.pt'\n","PATH_MODEL = f'{model_dir}/checkpoints/check.pt'\n","model, input_size = initialize_model()\n","#model.load_state_dict(torch.load(PATH_MODEL), strict=False)\n","\n","def get_prediction(model, x):\n","    pred_label = model(x).argmax()\n","    preds = F.softmax(model(torch.rand(1,3,224,224)))\n","    return pred_label, preds\n","\n","\n","x = torch.rand(1,3,224,224)\n","\n","get_prediction(model, x)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["(tensor(0), tensor([[0.7410, 0.2590]], grad_fn=<SoftmaxBackward>))"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"cOEmYLtUuVPC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1e6d70bdb21c4e8c9e3d732f5fdb014b","424e99b9dea84a89abf5619ff5a6e87c","f908f9a409e74a7abd83983d48c6c080","f7a0b87b5de344aa90229373b554387f","d8e1123cf3b6495c94b08d1abc831859","4e99966b35f74de48e90d0847cf1100d","b8e457a2474c419091c1c517dd240bfd","dea8b4aa2d3342f486870b4003c2f859"]},"outputId":"32692bc6-4992-4428-cc7a-de026f1d6b16","executionInfo":{"status":"ok","timestamp":1586784740847,"user_tz":-330,"elapsed":53613,"user":{"displayName":"Ishan Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjC7HQ3Ze_4nwWEuNncSdbObEvv62uZyisKn7YN=s64","userId":"07222164059474910029"}}},"source":["\n","import time\n","import os\n","import copy\n","import torch\n","import random\n","import torchvision\n","import pandas as pd\n","import torch.nn as nn\n","import seaborn as sns\n","from PIL import Image\n","from glob import glob\n","import torch.optim as optim\n","import torchvision.models as m\n","import matplotlib.pyplot as plt\n","from torchvision import transforms, datasets\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)\n","random.seed = 2020\n","\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","\n","from Augmentor import Pipeline\n","DIR = os.getcwd()\n","\n","p = Pipeline(source_directory=f'{DIR}/gdrive/My Drive/yo/datasets/covid19_vs_normal/xray_dataset/val',output_directory=f'{DIR}/gdrive/My Drive/yo/datasets/covid19_vs_normal/xray_dataset/test')\n","\n","#Pipeline\n","p.rotate(probability=0.5, max_left_rotation=15, max_right_rotation=15)\n","p.zoom(probability=0.7, min_factor=1.1, max_factor=1.2)\n","p.flip_left_right(probability=0.2)\n","\n","p.sample(200)\n","\n","\n","# Number of classes in the dataset\n","num_classes = 2\n","\n","# Batch size for training (change depending on how much memory you have)\n","batch_size = 8\n","\n","# Number of epochs to train for\n","num_epochs = 15\n","\n","# Flag for feature extracting. When False, we finetune the whole model,\n","#   when True we only update the reshaped layer params\n","feature_extract = True\n","\n","def initialize_model(num_classes, feature_extract, use_pretrained=True):\n","    model_ft = None\n","    input_size = 0\n","    \n","    model_ft = m.resnet50(pretrained=use_pretrained)\n","    set_parameter_requires_grad(model_ft, feature_extract)\n","    num_ftrs = model_ft.fc.in_features\n","    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","    input_size = 224\n","    \n","    return model_ft, input_size\n","\n","\n","def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet50\n","        \"\"\"\n","        model_ft = m.resnet50(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"alexnet\":\n","        \"\"\" Alexnet\n","        \"\"\"\n","        model_ft = m.alexnet(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"vgg\":\n","        \"\"\" VGG11_bn\n","        \"\"\"\n","        model_ft = m.vgg11_bn(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"squeezenet\":\n","        \"\"\" Squeezenet\n","        \"\"\"\n","        model_ft = m.squeezenet1_0(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n","        model_ft.num_classes = num_classes\n","        input_size = 224\n","\n","    elif model_name == \"densenet\":\n","        \"\"\" Densenet\n","        \"\"\"\n","        model_ft = m.densenet121(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier.in_features\n","        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"inception\":\n","        \"\"\" Inception v3\n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = m.inception_v3(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        num_ftrs = model_ft.AuxLogits.fc.in_features\n","        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","        input_size = 299\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","\n","    return model_ft, input_size\n","\n","\n","models = ['resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet', 'inception']\n","model_name = models[0]\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n","\n","# Print the model we just instantiated\n","print(model_ft)\n","\n","# Data augmentation and normalization for training\n","# Just normalization for validation\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(input_size),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.CenterCrop(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.CenterCrop(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","}\n","\n","\n","\n","def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n","    since = time.time()\n","\n","    val_acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","\n","                device = torch.device(\"cpu\")\n","                #inputs = inputs.to(device)\n","                #labels = labels.to(device)\n","\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history\n","\n","import os\n","DIR = os.getcwd()\n","data_dir = DIR+'/gdrive/My Drive/yo/datasets/covid19_vs_normal/xray_dataset'\n","# Create training and validation datasets\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n","# Create training and validation dataloaders\n","dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=0) for x in ['train', 'val', 'test']}\n","\n","# Detect if we have a GPU available\n","#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","# Send the model to GPU\n","model_ft = model_ft.to(device)\n","\n","# Gather the parameters to be optimized/updated in this run. If we are\n","#  finetuning we will be updating all parameters. However, if we are\n","#  doing feature extract method, we will only update the parameters\n","#  that we have just initialized, i.e. the parameters with requires_grad\n","#  is True.True.\n","params_to_update = model_ft.parameters()\n","print(\"Params to learn:\")\n","if feature_extract:\n","    params_to_update = []\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\",name)\n","else:\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            print(\"\\t\",name)\n","\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n","\n","\n","criterion = nn.CrossEntropyLoss()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["PyTorch Version:  1.4.0\n","Torchvision Version:  0.5.0\n"],"name":"stdout"},{"output_type":"stream","text":["\rExecuting Pipeline:   0%|          | 0/200 [00:00<?, ? Samples/s]"],"name":"stderr"},{"output_type":"stream","text":["Initialised with 22 image(s) found.\n","Output directory set to /content/gdrive/My Drive/yo/datasets/covid19_vs_normal/xray_dataset/test."],"name":"stdout"},{"output_type":"stream","text":["Processing <PIL.JpegImagePlugin.JpegImageFile image mode=L size=1802x1422 at 0x7F2C4ED60898>: 100%|██████████| 200/200 [00:40<00:00,  4.89 Samples/s]\n","Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e6d70bdb21c4e8c9e3d732f5fdb014b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=102502400), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",")\n","Params to learn:\n","\t fc.weight\n","\t fc.bias\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P18ghZBzw48v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"54d76d6e-97dc-462f-8862-737c820862f5","executionInfo":{"status":"ok","timestamp":1586784991196,"user_tz":-330,"elapsed":246811,"user":{"displayName":"Ishan Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjC7HQ3Ze_4nwWEuNncSdbObEvv62uZyisKn7YN=s64","userId":"07222164059474910029"}}},"source":["# Train and evaluate\n","model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Epoch 0/14\n","----------\n","train Loss: 0.6818 Acc: 0.6000\n","val Loss: 0.8458 Acc: 0.5000\n","\n","Epoch 1/14\n","----------\n","train Loss: 0.6631 Acc: 0.5750\n","val Loss: 0.7841 Acc: 0.5000\n","\n","Epoch 2/14\n","----------\n","train Loss: 0.7051 Acc: 0.5500\n","val Loss: 0.5646 Acc: 0.8182\n","\n","Epoch 3/14\n","----------\n","train Loss: 0.5004 Acc: 0.8500\n","val Loss: 0.6626 Acc: 0.5000\n","\n","Epoch 4/14\n","----------\n","train Loss: 0.4885 Acc: 0.8250\n","val Loss: 0.4359 Acc: 0.9091\n","\n","Epoch 5/14\n","----------\n","train Loss: 0.3957 Acc: 0.8750\n","val Loss: 0.4041 Acc: 0.9091\n","\n","Epoch 6/14\n","----------\n","train Loss: 0.3564 Acc: 0.9250\n","val Loss: 0.4549 Acc: 0.6364\n","\n","Epoch 7/14\n","----------\n","train Loss: 0.3243 Acc: 0.9500\n","val Loss: 0.2959 Acc: 0.9545\n","\n","Epoch 8/14\n","----------\n","train Loss: 0.3104 Acc: 0.8750\n","val Loss: 0.2528 Acc: 0.9545\n","\n","Epoch 9/14\n","----------\n","train Loss: 0.2193 Acc: 0.9750\n","val Loss: 0.2234 Acc: 1.0000\n","\n","Epoch 10/14\n","----------\n","train Loss: 0.3700 Acc: 0.8500\n","val Loss: 0.1910 Acc: 1.0000\n","\n","Epoch 11/14\n","----------\n","train Loss: 0.2725 Acc: 0.9250\n","val Loss: 0.2027 Acc: 0.9545\n","\n","Epoch 12/14\n","----------\n","train Loss: 0.2403 Acc: 0.9000\n","val Loss: 0.1459 Acc: 1.0000\n","\n","Epoch 13/14\n","----------\n","train Loss: 0.2325 Acc: 0.9250\n","val Loss: 0.1358 Acc: 1.0000\n","\n","Epoch 14/14\n","----------\n","train Loss: 0.2938 Acc: 0.9000\n","val Loss: 0.1182 Acc: 1.0000\n","\n","Training complete in 4m 6s\n","Best val Acc: 1.000000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cK4LVfW8vq9v","colab_type":"code","colab":{}},"source":["import os\n","DIR = os.getcwd()\n","model_dir = DIR + '/gdrive/My Drive/yo/datasets/covid19_vs_normal'\n","#torch.save(model_ft.state_dict(), f'{model_dir}/checkpoints/chk_resnet_50_epoch_{14}.pt')\n","torch.save(model_ft.state_dict(), f'{model_dir}/checkpoints/check.pt')\n","input_tensor_save = torch.rand(1,3,224,224,dtype=torch.float)\n","script_model = torch.jit.trace(model_ft.cpu(),input_tensor_save)\n","script_model.save(f'{model_dir}/checkpoints/cornet.pt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mjn0r1OsvsKC","colab_type":"code","colab":{}},"source":["def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n","    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n","    \n","    Arguments\n","    ---------\n","    confusion_matrix: numpy.ndarray\n","        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n","        Similarly constructed ndarrays can also be used.\n","    class_names: list\n","        An ordered list of class names, in the order they index the given confusion matrix.\n","    figsize: tuple\n","        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n","        the second determining the vertical size. Defaults to (10,7).\n","    fontsize: int\n","        Font size for axes labels. Defaults to 14.\n","        \n","    Returns\n","    -------\n","    matplotlib.figure.Figure\n","        The resulting confusion matrix figure\n","    \"\"\"\n","    df_cm = pd.DataFrame(\n","        confusion_matrix, index=class_names, columns=class_names, \n","    )\n","    fig = plt.figure(figsize=figsize)\n","    try:\n","        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n","    except ValueError:\n","        raise ValueError(\"Confusion matrix values must be integers.\")\n","    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n","    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    return fig\n","\n","images, labels = zip(*dataloaders_dict['test'].dataset.imgs)\n","shape = (len(images), 3, 224,224)\n","\n","resize = data_transforms['test']\n","x_test = torch.zeros(shape)\n","target = torch.Tensor(labels).long()\n","for idx, image in enumerate(images):\n","    x_test[idx,:,:,:] = resize(Image.open(image).convert('RGB'))\n","    \n","nb_samples = 10\n","nb_classes = 2\n","#output = model_ft(x_test.cuda())\n","output = model_ft(x_test)\n","pred = torch.argmax(output, 1)\n","\n","# compute the confusion matrix and and use it to derive the raw\n","# accuracy, sensitivity, and specificity\n","cm = confusion_matrix(target, pred.cpu())\n","total = sum(sum(cm))\n","acc = (cm[0, 0] + cm[1, 1]) / total\n","sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n","specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n","# show the confusion matrix, accuracy, sensitivity, and specificity\n","print(\"acc: {:.4f}\".format(acc))\n","print(\"sensitivity: {:.4f}\".format(sensitivity))\n","print(\"specificity: {:.4f}\".format(specificity))\n","\n","\n","# make predictions on the testing set\n","class_names = ['covid','normal']\n","print(\"[INFO] evaluating network...\")\n","print(classification_report(target, pred.cpu(),target_names=class_names))\n","\n","print_confusion_matrix(cm, class_names, figsize = (10,7), fontsize=14)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p69POTtEpG_7","colab_type":"code","colab":{}},"source":["from google.colab import files\n","import os\n","DIR = os.getcwd()\n","model_dir = DIR + '/gdrive/My Drive/yo/datasets/covid19_vs_normal'\n","PATH_MODEL = f'{model_dir}/checkpoints/cornet.pt'\n","files.download(PATH_MODEL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ywhwnnvkvv2p","colab_type":"code","colab":{}},"source":["from google.colab import files\n","import os\n","DIR = os.getcwd()\n","model_dir = DIR + '/gdrive/My Drive/yo/datasets/covid19_vs_normal'\n","PATH_MODEL = f'{model_dir}/checkpoints/cornet.pt'\n","files.download(PATH_MODEL)"],"execution_count":0,"outputs":[]}]}